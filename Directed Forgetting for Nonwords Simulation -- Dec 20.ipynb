{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1eefa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import string\n",
    "from collections import Counter\n",
    "import scipy.stats\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05298b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for R-squared\n",
    "def r_sq(x, y):\n",
    "    cor = scipy.stats.pearsonr(x, y)\n",
    "    return cor[0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c06e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_second(x):\n",
    "    return x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64e3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalize all vectors in a matrix\n",
    "def normalize_matrix(mat):\n",
    "    sq_mat = mat**2\n",
    "    sum_sq = np.sum(sq_mat, axis=1)\n",
    "    mag = np.sqrt(sum_sq)\n",
    "    mag[mag == 0] = 1\n",
    "    normed_mat = np.transpose((np.transpose(mat) / mag))\n",
    "    return normed_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd721913",
   "metadata": {},
   "source": [
    "Make Orthography vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfc8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tasa word list and nonwords\n",
    "with open('tasa_word_list.txt', 'r') as f:\n",
    "    df = f.readlines()\n",
    "all_words = [i.strip() for i in df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1d39e",
   "metadata": {},
   "source": [
    "    SERIOL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f20ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get bigrams and weights for seriol2\n",
    "def extract_bigrams(word, weights=[1, 0.7, 0.5]):\n",
    "    seriol_word = '*' + word + '*'\n",
    "    bigrams = []\n",
    "    for i in range(0, len(seriol_word)):\n",
    "        for x in range(1, 4):\n",
    "            if (i+x) < len(seriol_word):\n",
    "                bigrams.append((seriol_word[i] + seriol_word[i+x], weights[x-1]))\n",
    "    bigrams = [i for i in bigrams if i[0]!='**']\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef4e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all unique bigrams\n",
    "unique_strings = []\n",
    "for i in all_words:\n",
    "    unique_strings.extend(list(set([x[0] for x in extract_bigrams(i)])))\n",
    "unique_strings = list(set(unique_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abfb0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blank matrix\n",
    "ortho_matrix = np.zeros((len(all_words), len(unique_strings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "482299ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for all words\n",
    "ortho_dic = {}\n",
    "for i in range(0, len(all_words)):\n",
    "    ortho_dic[all_words[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca84241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for all bigrams\n",
    "string_dic = {}\n",
    "for i in range(0, len(unique_strings)):\n",
    "    string_dic[unique_strings[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d86f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word-by-bigram matrix\n",
    "for i in all_words:\n",
    "    for x in extract_bigrams(i):\n",
    "        ortho_matrix[ortho_dic[i]][string_dic[x[0]]] += x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab7d7898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for singular value decomposition (SVD) (300 dimensions)\n",
    "svd = TruncatedSVD(n_components=300, algorithm='arpack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3a4c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform SVD (takes a little time)\n",
    "ortho_matrix = svd.fit_transform(ortho_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7341a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize matrix\n",
    "seriol_matrix = normalize_matrix(ortho_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d0eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for finding orthographic neighbors (not required to simulate memory)\n",
    "def find_neighbors(x, dic, mat, n=30):\n",
    "    simvec = mat[dic[x]] @ np.transpose(mat)\n",
    "    sim_list = list(zip([i for i in dic], simvec))\n",
    "    sim_list.sort(key=sel_second, reverse=True)\n",
    "    return sim_list[1:(n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67d14792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tice', 0.8526681804155765),\n",
       " ('stice', 0.7199674626781277),\n",
       " ('ice', 0.6876699564332858),\n",
       " ('titmice', 0.6762183010593561),\n",
       " ('wince', 0.670086823466045),\n",
       " ('twite', 0.6599433981335214),\n",
       " ('twine', 0.6591313007351359),\n",
       " ('tic', 0.6570869048941395),\n",
       " ('wic', 0.6299350675702022),\n",
       " ('wickie', 0.6247972531746894),\n",
       " ('weicek', 0.62109491426506),\n",
       " ('notice', 0.6147037932144468),\n",
       " ('entice', 0.6141692284298366),\n",
       " ('tie', 0.6056609354283639),\n",
       " ('vice', 0.6054667344390299),\n",
       " ('triced', 0.6040850903750908),\n",
       " ('tricep', 0.6029067954315054),\n",
       " ('twitched', 0.6016352809941627),\n",
       " ('hice', 0.5989964481380832),\n",
       " ('thrice', 0.5987590930625922),\n",
       " ('pice', 0.5972353277017325),\n",
       " ('dice', 0.595743974145482),\n",
       " ('thicke', 0.595338083728224),\n",
       " ('mice', 0.5933888051856302),\n",
       " ('icie', 0.5917210271172807),\n",
       " ('lice', 0.5893059746780324),\n",
       " ('wickwire', 0.5877997658232164),\n",
       " ('twinkie', 0.5869434704395182),\n",
       " ('twitches', 0.5862649688015245),\n",
       " ('nice', 0.5862296321932584)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example \n",
    "find_neighbors('twice', ortho_dic, seriol_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6ab1a",
   "metadata": {},
   "source": [
    "    OrBEAGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edd3d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get letter strings according to terminal-relative encoding (Cox et al., 2011)\n",
    "def get_letter_strings(word):\n",
    "    singles = [i for i in word]\n",
    "    bigrams = list(zip(word[:-1], word[1:]))\n",
    "    bigrams = [\"\".join(i) for i in bigrams]\n",
    "    t1 = []\n",
    "    if len(word)>2:\n",
    "        t1.append(bigrams[0])\n",
    "        t1.extend(tuple(product(*[[singles[0] + '_'], singles[2:]])))\n",
    "        t1.extend(tuple(product(*[singles[:-2], ['_' + singles[-1]]]))[1:])\n",
    "        t1.append(bigrams[-1])\n",
    "        t1.append((singles[0], bigrams[1]))\n",
    "        t1.extend(tuple(product(*[[singles[0] + '_'], bigrams[2:]])))\n",
    "        t1.extend(tuple(product(*[bigrams[:-2], ['_' + singles[-1]]])))\n",
    "        t1.append((bigrams[-2], singles[-1]))\n",
    "        t1 = [\"\".join(i) for i in t1]\n",
    "    return singles + bigrams + t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9119889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an object with all letters and a space symbol\n",
    "alphabet = string.ascii_lowercase + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "025d0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a random vector for each letter (and space symbol)\n",
    "letter_dic = {}\n",
    "for i in alphabet:\n",
    "    letter_dic[i] = np.random.normal(0, 1/np.sqrt(1024), 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdf2b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for circular convolution\n",
    "def conv_circ(signal, ker):\n",
    "    return np.real(np.fft.ifft(np.fft.fft(signal)*np.fft.fft(ker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "074cb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make arrays to randomly permute letters to the left and right\n",
    "num = [i for i in range(0, 1024)]\n",
    "E1 = np.random.permutation(num)\n",
    "E2 = np.random.permutation(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20bfd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a vector representation for each substring\n",
    "def make_string_vec(string):\n",
    "    s_vec = letter_dic[string[0]].copy()\n",
    "    if len(string) > 1:\n",
    "        for i in string[1:]:\n",
    "            s_vec = conv_circ(s_vec[E1], letter_dic[i][E2])\n",
    "    return s_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d23a48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary for all words\n",
    "ortho_dic = {}\n",
    "for i in range(0, len(all_words)):\n",
    "    ortho_dic[all_words[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f02fa039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vector representations for all words (make vectors for substrings and add these up) (takes a while)\n",
    "ortho_matrix = []\n",
    "for i in all_words:\n",
    "    word_vec = np.zeros(1024)\n",
    "    sub = get_letter_strings(i)\n",
    "    for x in sub:\n",
    "        word_vec += make_string_vec(x)\n",
    "    ortho_matrix.append(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "150c437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to numpy matrix\n",
    "ortho_matrix = np.array(ortho_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f78e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize matrix\n",
    "orbeagle_matrix = normalize_matrix(ortho_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17148ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find orthographic neighbors\n",
    "def find_neighbors(x, dic, mat, n=30):\n",
    "    simvec = mat[dic[x]] @ np.transpose(mat)\n",
    "    sim_list = list(zip([i for i in dic], simvec))\n",
    "    sim_list.sort(key=sel_second, reverse=True)\n",
    "    return sim_list[1:(n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea6e34fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twine', 0.6706159011044597),\n",
       " ('twite', 0.6444096467805196),\n",
       " ('tice', 0.6106410993325848),\n",
       " ('twinge', 0.6051627973331898),\n",
       " ('titmice', 0.5985045922500533),\n",
       " ('twinkie', 0.5873305815328165),\n",
       " ('thrice', 0.5786932973046923),\n",
       " ('twopence', 0.5719920921611105),\n",
       " ('wince', 0.5588220857487379),\n",
       " ('ice', 0.5525291947522528),\n",
       " ('twinkle', 0.54649902619543),\n",
       " ('twiglike', 0.5426615341497039),\n",
       " ('artifice', 0.494531575404631),\n",
       " ('thwickle', 0.48841685397636825),\n",
       " ('stice', 0.4793642547357937),\n",
       " ('twiddle', 0.4712660267467289),\n",
       " ('lattice', 0.4674149489816697),\n",
       " ('twig', 0.46524137120745435),\n",
       " ('tw', 0.4649184292348183),\n",
       " ('juice', 0.46326267837925417),\n",
       " ('timepiece', 0.4628480911544469),\n",
       " ('injustice', 0.46132469431096024),\n",
       " ('intertwine', 0.4605508056378591),\n",
       " ('vice', 0.4585236171298746),\n",
       " ('hotwire', 0.4559811161261165),\n",
       " ('dice', 0.44945319063383093),\n",
       " ('twines', 0.44938711421228766),\n",
       " ('withe', 0.4485482277971237),\n",
       " ('armistice', 0.44756911967791757),\n",
       " ('witte', 0.44489765081652954)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "find_neighbors('twice', ortho_dic, orbeagle_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5851c6",
   "metadata": {},
   "source": [
    "Directed Forgetting Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ca9a1",
   "metadata": {},
   "source": [
    "    Load experiment 1 stimuli lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ea3f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = []\n",
    "with open('exp1testlistA.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for i in csvreader:\n",
    "        test_a.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0eb7f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_b = []\n",
    "with open('exp1testlistB.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for i in csvreader:\n",
    "        test_b.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81ce9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_c = []\n",
    "with open('exp1testlistC.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for i in csvreader:\n",
    "        test_c.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ef61e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d = []\n",
    "with open('exp1testlistD.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for i in csvreader:\n",
    "        test_d.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb7404",
   "metadata": {},
   "source": [
    "    Load experiment 2 stimuli lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cde508bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a_2 = []\n",
    "with open('exp2testlistA.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for i in csvreader:\n",
    "        test_a_2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a21fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_b_2 = []\n",
    "with open('exp2testlistB.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for i in csvreader:\n",
    "        test_b_2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e394148",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_c_2 = []\n",
    "with open('exp2testlistC.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for i in csvreader:\n",
    "        test_c_2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "028fc9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d_2 = []\n",
    "with open('exp2testlistD.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for i in csvreader:\n",
    "        test_d_2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4e9bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to set up stimuli list for running simulation\n",
    "def get_stim_list(df):\n",
    "    clean_df = df[1:]\n",
    "    r_old = [i[1].lower() for i in clean_df if i[2]=='R' and i[4]=='old']\n",
    "    f_old = [i[1].lower() for i in clean_df if i[2]=='F' and i[4]=='old']\n",
    "    r_new = [i[1].lower() for i in clean_df if i[2]=='R' and i[4]=='new']\n",
    "    f_new = [i[1].lower() for i in clean_df if i[2]=='F' and i[4]=='new']\n",
    "    new_new = [i[1].lower() for i in clean_df if i[2]=='New' and i[4]=='new']\n",
    "    return r_old + f_old + r_new + f_new + new_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57f18ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make matrix for a subset of items (e.g., items to be encoded or items on recognition test)\n",
    "def make_matrix(word_list, ortho_matrix):\n",
    "    mem = []\n",
    "    for i in word_list:\n",
    "        mem.append(ortho_matrix[ortho_dic[i]])\n",
    "    return np.array(mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7d438f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for retrieval\n",
    "def echo_intensity(probes, memory, tau=3):\n",
    "    normed_memory = normalize_matrix(memory)\n",
    "    similarities = probes @ np.transpose(normed_memory)\n",
    "    if tau == 2:\n",
    "        activations = similarities*(abs(similarities))\n",
    "    elif tau == 4:\n",
    "        activations = similarities*(abs(similarities))*similarities*(abs(similarities))\n",
    "    elif tau == 6:\n",
    "        activations = similarities*(abs(similarities))*similarities*(abs(similarities))*similarities*(abs(similarities))\n",
    "    else:\n",
    "        activations = similarities**tau\n",
    "    activations = np.sum(activations, axis=1)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ffc841f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'F', 'mean/sd', ['R old', 'F old', 'R rel', 'F rel', 'Unrel'], 'R-squared']\n",
      "\n",
      "\n",
      "[0.01, 0.67, 'means:', [0.78, 0.57, 0.58, 0.43, 0.14], 0.9824]\n",
      "[0.01, 0.67, 'sds:', [0.08, 0.09, 0.11, 0.11, 0.07], 'N/A']\n",
      "\n",
      "\n",
      "[0.03, 0.67, 'means:', [0.95, 0.64, 0.57, 0.25, 0.01], 0.9149]\n",
      "[0.03, 0.67, 'sds:', [0.04, 0.07, 0.12, 0.09, 0.01], 'N/A']\n",
      "\n",
      "\n",
      "[0.05, 0.67, 'means:', [0.98, 0.66, 0.54, 0.17, 0.0], 0.8509]\n",
      "[0.05, 0.67, 'sds:', [0.02, 0.06, 0.12, 0.08, 0.0], 'N/A']\n",
      "\n",
      "\n",
      "[0.1, 0.67, 'means:', [1.0, 0.7, 0.52, 0.08, 0.0], 0.7835]\n",
      "[0.1, 0.67, 'sds:', [0.01, 0.06, 0.13, 0.07, 0.0], 'N/A']\n",
      "\n",
      "\n",
      "[0.01, 0.8, 'means:', [0.75, 0.63, 0.54, 0.46, 0.12], 0.9217]\n",
      "[0.01, 0.8, 'sds:', [0.08, 0.09, 0.12, 0.12, 0.06], 'N/A']\n",
      "\n",
      "\n",
      "[0.03, 0.8, 'means:', [0.91, 0.73, 0.46, 0.27, 0.0], 0.818]\n",
      "[0.03, 0.8, 'sds:', [0.05, 0.06, 0.12, 0.1, 0.01], 'N/A']\n",
      "\n",
      "\n",
      "[0.05, 0.8, 'means:', [0.95, 0.75, 0.4, 0.18, 0.0], 0.7355]\n",
      "[0.05, 0.8, 'sds:', [0.04, 0.06, 0.12, 0.09, 0.0], 'N/A']\n",
      "\n",
      "\n",
      "[0.1, 0.8, 'means:', [0.98, 0.8, 0.32, 0.1, 0.0], 0.6273]\n",
      "[0.1, 0.8, 'sds:', [0.02, 0.05, 0.12, 0.07, 0.0], 'N/A']\n"
     ]
    }
   ],
   "source": [
    "# Run Experiment 1 simulation with OrBEAGLE representations\n",
    "emp_data = np.array([0.72619048, 0.52947846, 0.61111111, 0.42630385, 0.16326531]) # empirical data for computing r-squared\n",
    "ls = [.01, .03, .05, .1] # learning rate\n",
    "fs = [2/3, 4/5] # directed forgetting parameter\n",
    "t = 3 # tau parameter\n",
    "p_old = 50 # percent of items to be classifed \"OLD\"\n",
    "print(['L', 'F', 'mean/sd', ['R old', 'F old', 'R rel', 'F rel', 'Unrel'], 'R-squared'])\n",
    "for F in fs:\n",
    "    for l in ls:\n",
    "        f = l * F\n",
    "        sim_list = []\n",
    "        for s in range(0, 800):\n",
    "            if s <= 200:\n",
    "                stims = get_stim_list(test_a)\n",
    "            elif s <= 400:\n",
    "                stims = get_stim_list(test_b)\n",
    "            elif s <= 600:\n",
    "                stims = get_stim_list(test_c)\n",
    "            else:\n",
    "                stims = get_stim_list(test_d)\n",
    "            memory = make_matrix(stims[0:72], orbeagle_matrix) # get vectors for encoded items\n",
    "            memory[0:36] = memory[0:36] * np.random.choice([0, 1], size=(36, len(memory[0])), p=[1-l, l]) # randomly encode elements with probability L\n",
    "            memory[36:72] = memory[36:72] * np.random.choice([0, 1], size=(36, len(memory[0])), p=[1-f, f]) # randomly encode elements with probability L*F\n",
    "            probes = make_matrix(stims, orbeagle_matrix) # get vectors for recognition probes\n",
    "            familiarities = echo_intensity(probes, memory, tau=t) # retrival: compute familairity for all probes\n",
    "            criterion = np.percentile(familiarities, 100-p_old) # set criterion\n",
    "            r_old = familiarities[0:36]\n",
    "            f_old = familiarities[36:72]\n",
    "            r_new = familiarities[72:90]\n",
    "            f_new = familiarities[90:108]\n",
    "            new_cat = familiarities[108:144]\n",
    "            r_old_hits = np.sum(r_old > criterion) / 36\n",
    "            f_old_hits = np.sum(f_old > criterion) / 36\n",
    "            r_new_hits = np.sum(r_new > criterion) / 18\n",
    "            f_new_hits = np.sum(f_new > criterion) / 18\n",
    "            new_cat_hits = np.sum(new_cat > criterion) / 36\n",
    "            sim_list.append([r_old_hits, f_old_hits, r_new_hits, f_new_hits, new_cat_hits])\n",
    "        means = np.mean(sim_list, axis=0)\n",
    "        r_sq_value = r_sq(means, emp_data)\n",
    "        means = means.tolist()\n",
    "        means = [round(i, 2) for i in means]\n",
    "        sds = np.std(sim_list, axis=0, ddof=1)\n",
    "        sds = sds.tolist()\n",
    "        sds = [round(i, 2) for i in sds]\n",
    "        print('\\n')\n",
    "        print([l, round(F, 2), 'means:', means, round(r_sq_value, 4)])\n",
    "        print([l, round(F, 2), 'sds:', sds, 'N/A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab1ef333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'F', 'mean/sd', ['R old', 'F old', 'R rel', 'F rel', 'Unrel'], 'R-squared']\n",
      "\n",
      "\n",
      "[0.1, 0.67, 'means:', [0.78, 0.54, 0.63, 0.44, 0.15], 0.9987]\n",
      "[0.1, 0.67, 'sds:', [0.11, 0.11, 0.14, 0.12, 0.08], 'N/A']\n",
      "\n",
      "\n",
      "[0.15, 0.67, 'means:', [0.85, 0.54, 0.67, 0.4, 0.08], 0.9949]\n",
      "[0.15, 0.67, 'sds:', [0.09, 0.09, 0.13, 0.12, 0.06], 'N/A']\n",
      "\n",
      "\n",
      "[0.2, 0.67, 'means:', [0.89, 0.53, 0.69, 0.37, 0.05], 0.9879]\n",
      "[0.2, 0.67, 'sds:', [0.08, 0.09, 0.13, 0.11, 0.04], 'N/A']\n",
      "\n",
      "\n",
      "[0.3, 0.67, 'means:', [0.94, 0.53, 0.72, 0.31, 0.01], 0.9737]\n",
      "[0.3, 0.67, 'sds:', [0.05, 0.07, 0.12, 0.1, 0.02], 'N/A']\n",
      "\n",
      "\n",
      "[0.1, 0.8, 'means:', [0.74, 0.6, 0.59, 0.47, 0.13], 0.9664]\n",
      "[0.1, 0.8, 'sds:', [0.11, 0.11, 0.14, 0.13, 0.07], 'N/A']\n",
      "\n",
      "\n",
      "[0.15, 0.8, 'means:', [0.8, 0.61, 0.6, 0.44, 0.07], 0.9758]\n",
      "[0.15, 0.8, 'sds:', [0.1, 0.1, 0.13, 0.12, 0.06], 'N/A']\n",
      "\n",
      "\n",
      "[0.2, 0.8, 'means:', [0.83, 0.62, 0.61, 0.41, 0.04], 0.9798]\n",
      "[0.2, 0.8, 'sds:', [0.09, 0.1, 0.13, 0.12, 0.04], 'N/A']\n",
      "\n",
      "\n",
      "[0.3, 0.8, 'means:', [0.89, 0.62, 0.61, 0.37, 0.01], 0.9738]\n",
      "[0.3, 0.8, 'sds:', [0.07, 0.08, 0.12, 0.11, 0.02], 'N/A']\n"
     ]
    }
   ],
   "source": [
    "# Run Experiment 1 simulation with OrBEAGLE representations\n",
    "emp_data = np.array([0.72619048, 0.52947846, 0.61111111, 0.42630385, 0.16326531]) # empirical data for computing r-squared\n",
    "ls = [.1, .15, .2, .3] # learning rate\n",
    "fs = [2/3, 4/5] # directed forgetting parameter\n",
    "t = 3 # tau parameter\n",
    "p_old = 50 # percent of items to be classifed \"OLD\"\n",
    "print(['L', 'F', 'mean/sd', ['R old', 'F old', 'R rel', 'F rel', 'Unrel'], 'R-squared'])\n",
    "for F in fs:\n",
    "    for l in ls:\n",
    "        f = l * F\n",
    "        sim_list = []\n",
    "        for s in range(0, 800):\n",
    "            if s <= 200:\n",
    "                stims = get_stim_list(test_a)\n",
    "            elif s <= 400:\n",
    "                stims = get_stim_list(test_b)\n",
    "            elif s <= 600:\n",
    "                stims = get_stim_list(test_c)\n",
    "            else:\n",
    "                stims = get_stim_list(test_d)\n",
    "            memory = make_matrix(stims[0:72], seriol_matrix) # get vectors for encoded items\n",
    "            memory[0:36] = memory[0:36] * np.random.choice([0, 1], size=(36, len(memory[0])), p=[1-l, l]) # randomly encode elements with probability L\n",
    "            memory[36:72] = memory[36:72] * np.random.choice([0, 1], size=(36, len(memory[0])), p=[1-f, f]) # randomly encode elements with probability L*F\n",
    "            probes = make_matrix(stims, seriol_matrix) # get vectors for recognition probes\n",
    "            familiarities = echo_intensity(probes, memory, tau=t) # retrival: compute familairity for all probes\n",
    "            criterion = np.percentile(familiarities, 100-p_old) # set criterion\n",
    "            r_old = familiarities[0:36]\n",
    "            f_old = familiarities[36:72]\n",
    "            r_new = familiarities[72:90]\n",
    "            f_new = familiarities[90:108]\n",
    "            new_cat = familiarities[108:144]\n",
    "            r_old_hits = np.sum(r_old > criterion) / 36\n",
    "            f_old_hits = np.sum(f_old > criterion) / 36\n",
    "            r_new_hits = np.sum(r_new > criterion) / 18\n",
    "            f_new_hits = np.sum(f_new > criterion) / 18\n",
    "            new_cat_hits = np.sum(new_cat > criterion) / 36\n",
    "            sim_list.append([r_old_hits, f_old_hits, r_new_hits, f_new_hits, new_cat_hits])\n",
    "        means = np.mean(sim_list, axis=0)\n",
    "        r_sq_value = r_sq(means, emp_data)\n",
    "        means = means.tolist()\n",
    "        means = [round(i, 2) for i in means]\n",
    "        sds = np.std(sim_list, axis=0, ddof=1)\n",
    "        sds = sds.tolist()\n",
    "        sds = [round(i, 2) for i in sds]\n",
    "        print('\\n')\n",
    "        print([l, round(F, 2), 'means:', means, round(r_sq_value, 4)])\n",
    "        print([l, round(F, 2), 'sds:', sds, 'N/A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dafd58d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'F', 'mean/sd', ['R old', 'F old', 'R rel', 'F rel', 'Unrel'], 'R-squared']\n",
      "\n",
      "\n",
      "[0.01, 0.67, 'means:', [0.76, 0.57, 0.57, 0.42, 0.18], 0.9992]\n",
      "[0.01, 0.67, 'sds:', [0.08, 0.09, 0.12, 0.11, 0.08], 'N/A']\n",
      "\n",
      "\n",
      "[0.03, 0.67, 'means:', [0.95, 0.63, 0.58, 0.25, 0.01], 0.9677]\n",
      "[0.03, 0.67, 'sds:', [0.04, 0.07, 0.12, 0.09, 0.01], 'N/A']\n",
      "\n",
      "\n",
      "[0.05, 0.67, 'means:', [0.98, 0.65, 0.56, 0.17, 0.0], 0.9264]\n",
      "[0.05, 0.67, 'sds:', [0.02, 0.06, 0.12, 0.08, 0.0], 'N/A']\n",
      "\n",
      "\n",
      "[0.1, 0.67, 'means:', [1.0, 0.68, 0.55, 0.09, 0.0], 0.8777]\n",
      "[0.1, 0.67, 'sds:', [0.01, 0.06, 0.12, 0.07, 0.0], 'N/A']\n",
      "\n",
      "\n",
      "[0.01, 0.8, 'means:', [0.74, 0.61, 0.53, 0.45, 0.16], 0.9786]\n",
      "[0.01, 0.8, 'sds:', [0.08, 0.08, 0.12, 0.11, 0.07], 'N/A']\n",
      "\n",
      "\n",
      "[0.03, 0.8, 'means:', [0.91, 0.71, 0.48, 0.28, 0.0], 0.9341]\n",
      "[0.03, 0.8, 'sds:', [0.05, 0.07, 0.12, 0.1, 0.01], 'N/A']\n",
      "\n",
      "\n",
      "[0.05, 0.8, 'means:', [0.95, 0.74, 0.42, 0.19, 0.0], 0.8685]\n",
      "[0.05, 0.8, 'sds:', [0.04, 0.06, 0.11, 0.09, 0.0], 'N/A']\n",
      "\n",
      "\n",
      "[0.1, 0.8, 'means:', [0.98, 0.79, 0.34, 0.1, 0.0], 0.7801]\n",
      "[0.1, 0.8, 'sds:', [0.02, 0.05, 0.1, 0.07, 0.0], 'N/A']\n"
     ]
    }
   ],
   "source": [
    "# Run Experiment 2 simulation with OrBEAGLE representations\n",
    "emp_data = np.array([0.68026005, 0.53546099, 0.53073286, 0.41371158, 0.21394799]) # empirical data for computing r-squared\n",
    "ls = [.01, .03, .05, .1] # learning rate\n",
    "fs = [2/3, 4/5] # directed forgetting parameter\n",
    "t = 3 # tau parameter\n",
    "p_old = 50 # percent of items to be classifed \"OLD\"\n",
    "print(['L', 'F', 'mean/sd', ['R old', 'F old', 'R rel', 'F rel', 'Unrel'], 'R-squared'])\n",
    "for F in fs:\n",
    "    for l in ls:\n",
    "        f = l * F\n",
    "        sim_list = []\n",
    "        for s in range(0, 800):\n",
    "            if s <= 200:\n",
    "                stims = get_stim_list(test_a_2)\n",
    "            elif s <= 400:\n",
    "                stims = get_stim_list(test_b_2)\n",
    "            elif s <= 600:\n",
    "                stims = get_stim_list(test_c_2)\n",
    "            else:\n",
    "                stims = get_stim_list(test_d_2)\n",
    "            memory = make_matrix(stims[0:72], orbeagle_matrix) # get vectors for encoded items\n",
    "            memory[0:36] = memory[0:36] * np.random.choice([0, 1], size=(36, len(memory[0])), p=[1-l, l]) # randomly encode elements with probability L\n",
    "            memory[36:72] = memory[36:72] * np.random.choice([0, 1], size=(36, len(memory[0])), p=[1-f, f]) # randomly encode elements with probability L*F\n",
    "            probes = make_matrix(stims, orbeagle_matrix) # get vectors for recognition probes\n",
    "            familiarities = echo_intensity(probes, memory, tau=t) # retrival: compute familairity for all probes\n",
    "            criterion = np.percentile(familiarities, 100-p_old) # set criterion\n",
    "            r_old = familiarities[0:36]\n",
    "            f_old = familiarities[36:72]\n",
    "            r_new = familiarities[72:90]\n",
    "            f_new = familiarities[90:108]\n",
    "            new_cat = familiarities[108:144]\n",
    "            r_old_hits = np.sum(r_old > criterion) / 36\n",
    "            f_old_hits = np.sum(f_old > criterion) / 36\n",
    "            r_new_hits = np.sum(r_new > criterion) / 18\n",
    "            f_new_hits = np.sum(f_new > criterion) / 18\n",
    "            new_cat_hits = np.sum(new_cat > criterion) / 36\n",
    "            sim_list.append([r_old_hits, f_old_hits, r_new_hits, f_new_hits, new_cat_hits])\n",
    "        means = np.mean(sim_list, axis=0)\n",
    "        r_sq_value = r_sq(means, emp_data)\n",
    "        means = means.tolist()\n",
    "        means = [round(i, 2) for i in means]\n",
    "        sds = np.std(sim_list, axis=0, ddof=1)\n",
    "        sds = sds.tolist()\n",
    "        sds = [round(i, 2) for i in sds]\n",
    "        print('\\n')\n",
    "        print([l, round(F, 2), 'means:', means, round(r_sq_value, 4)])\n",
    "        print([l, round(F, 2), 'sds:', sds, 'N/A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11f22786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'F', 'mean/sd', ['R old', 'F old', 'R rel', 'F rel', 'Unrel'], 'R-squared']\n",
      "\n",
      "\n",
      "[0.1, 0.67, 'means:', [0.77, 0.52, 0.64, 0.44, 0.17], 0.9577]\n",
      "[0.1, 0.67, 'sds:', [0.09, 0.1, 0.13, 0.12, 0.09], 'N/A']\n",
      "\n",
      "\n",
      "[0.15, 0.67, 'means:', [0.85, 0.52, 0.68, 0.41, 0.09], 0.9571]\n",
      "[0.15, 0.67, 'sds:', [0.09, 0.1, 0.13, 0.12, 0.06], 'N/A']\n",
      "\n",
      "\n",
      "[0.2, 0.67, 'means:', [0.89, 0.52, 0.7, 0.38, 0.05], 0.956]\n",
      "[0.2, 0.67, 'sds:', [0.07, 0.09, 0.14, 0.11, 0.04], 'N/A']\n",
      "\n",
      "\n",
      "[0.3, 0.67, 'means:', [0.95, 0.52, 0.74, 0.32, 0.01], 0.94]\n",
      "[0.3, 0.67, 'sds:', [0.05, 0.07, 0.12, 0.1, 0.01], 'N/A']\n",
      "\n",
      "\n",
      "[0.1, 0.8, 'means:', [0.73, 0.58, 0.6, 0.48, 0.16], 0.976]\n",
      "[0.1, 0.8, 'sds:', [0.09, 0.1, 0.13, 0.13, 0.09], 'N/A']\n",
      "\n",
      "\n",
      "[0.15, 0.8, 'means:', [0.79, 0.6, 0.61, 0.46, 0.07], 0.9855]\n",
      "[0.15, 0.8, 'sds:', [0.09, 0.1, 0.13, 0.13, 0.06], 'N/A']\n",
      "\n",
      "\n",
      "[0.2, 0.8, 'means:', [0.84, 0.61, 0.62, 0.42, 0.03], 0.9944]\n",
      "[0.2, 0.8, 'sds:', [0.08, 0.09, 0.14, 0.12, 0.04], 'N/A']\n",
      "\n",
      "\n",
      "[0.3, 0.8, 'means:', [0.89, 0.61, 0.63, 0.36, 0.0], 0.9984]\n",
      "[0.3, 0.8, 'sds:', [0.07, 0.09, 0.15, 0.12, 0.01], 'N/A']\n"
     ]
    }
   ],
   "source": [
    "# Run Experiment 2 simulation with SERIOL2 representations\n",
    "emp_data = np.array([0.68026005, 0.53546099, 0.53073286, 0.41371158, 0.21394799]) # empirical data for computing r-squared\n",
    "ls = [.1, .15, .2, .3] # learning rate\n",
    "fs = [2/3, 4/5] # directed forgetting parameter\n",
    "t = 3 # tau parameter\n",
    "p_old = 50 # percent of items to be classifed \"OLD\"\n",
    "print(['L', 'F', 'mean/sd', ['R old', 'F old', 'R rel', 'F rel', 'Unrel'], 'R-squared'])\n",
    "for F in fs:\n",
    "    for l in ls:\n",
    "        f = l * F\n",
    "        sim_list = []\n",
    "        for s in range(0, 800):\n",
    "            if s <= 200:\n",
    "                stims = get_stim_list(test_a_2)\n",
    "            elif s <= 400:\n",
    "                stims = get_stim_list(test_b_2)\n",
    "            elif s <= 600:\n",
    "                stims = get_stim_list(test_c_2)\n",
    "            else:\n",
    "                stims = get_stim_list(test_d_2)\n",
    "            memory = make_matrix(stims[0:72], seriol_matrix) # get vectors for encoded items\n",
    "            memory[0:36] = memory[0:36] * np.random.choice([0, 1], size=(36, len(memory[0])), p=[1-l, l]) # randomly encode elements with probability L\n",
    "            memory[36:72] = memory[36:72] * np.random.choice([0, 1], size=(36, len(memory[0])), p=[1-f, f]) # randomly encode elements with probability L*F\n",
    "            probes = make_matrix(stims, seriol_matrix) # get vectors for recognition probes\n",
    "            familiarities = echo_intensity(probes, memory, tau=t) # retrival: compute familairity for all probes\n",
    "            criterion = np.percentile(familiarities, 100-p_old) # set criterion\n",
    "            r_old = familiarities[0:36]\n",
    "            f_old = familiarities[36:72]\n",
    "            r_new = familiarities[72:90]\n",
    "            f_new = familiarities[90:108]\n",
    "            new_cat = familiarities[108:144]\n",
    "            r_old_hits = np.sum(r_old > criterion) / 36\n",
    "            f_old_hits = np.sum(f_old > criterion) / 36\n",
    "            r_new_hits = np.sum(r_new > criterion) / 18\n",
    "            f_new_hits = np.sum(f_new > criterion) / 18\n",
    "            new_cat_hits = np.sum(new_cat > criterion) / 36\n",
    "            sim_list.append([r_old_hits, f_old_hits, r_new_hits, f_new_hits, new_cat_hits])\n",
    "        means = np.mean(sim_list, axis=0)\n",
    "        r_sq_value = r_sq(means, emp_data)\n",
    "        means = means.tolist()\n",
    "        means = [round(i, 2) for i in means]\n",
    "        sds = np.std(sim_list, axis=0, ddof=1)\n",
    "        sds = sds.tolist()\n",
    "        sds = [round(i, 2) for i in sds]\n",
    "        print('\\n')\n",
    "        print([l, round(F, 2), 'means:', means, round(r_sq_value, 4)])\n",
    "        print([l, round(F, 2), 'sds:', sds, 'N/A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c03d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
